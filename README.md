# ğŸ§  LLM Arena â€” GenAI Playground Web App

A full-stack web application that lets users interact with multiple LLMs (Large Language Models) using a unified UI. Built with React (frontend) and Node.js + Express (backend), the app integrates popular GenAI APIs and stores user interactions in MongoDB.

## âœ¨ Features

- ğŸ”Œ **Multi-Model Support** â€” Plug-and-play integration for:
  - Gemini (`gemini-1.5-flash-latest`)
  - Cohere
  - Mistral
  - Groq
- ğŸ’¬ **Prompt-Response Chat UI**
  - Users can submit prompts dynamically
  - Instant LLM-generated responses
- ğŸ—‚ï¸ **Session-Based Chat (Upcoming)**
  - Maintain context for each model
  - View past prompt-response history
- ğŸ› ï¸ **MongoDB Persistence**
  - Stores prompts, responses, timestamps, and model info
- ğŸŒ **API Key Management**
  - Secured via `.env` configuration

## ğŸ”§ Tech Stack

| Layer       | Technology         |
|------------|--------------------|
| Frontend   | React              |
| Backend    | Node.js, Express   |
| Database   | MongoDB (Mongoose) |
| AI Models  | Gemini, Groq, Cohere, Mistral |
| Deployment | (Upcoming)         |

## ğŸ“¦ Folder Structure

/llm-arena â†’ React frontend
/llm-arena-backend
â”œâ”€â”€ /routes â†’ Model routers (e.g., gemini.js)
â”œâ”€â”€ /models â†’ Mongoose schemas
â””â”€â”€ index.js â†’ Express server setup


## ğŸ“Œ How to Use

1. Clone the repo  
2. Set up your `.env` in `/llm-arena-backend`:

GEMINI_API_KEY=your_key
MONGODB_URI=your_connection_string

3. Run backend:

cd llm-arena-backend
npm install
node index.js

4. Run frontend:

cd client
npm install
npm start

ğŸš€ Planned Features
 Cohere, Mistral, Groq integration

 Session-based chat for each model

 Chat history display

 UI polish (tabs, loaders, bubbles)

 Vercel/Render deployment

ğŸ‘©â€ğŸ’» Built By
Neha Gururaj
Frontend Developer | Exploring GenAI | Career Switch Aspirant
